"""
é›†æˆç¤ºä¾‹: å®Œæ•´çš„å¤šæ¨¡æ€å¯è¿½é—®æ¨ç†æµç¨‹
å±•ç¤ºå¦‚ä½•ç»“åˆVLMèåˆ + å¤šæ¨¡æ€çŠ¶æ€ç®¡ç†å®ç°ç«èµ›çº§æ¶æ„
"""
from typing import Dict, Optional
from ai_engine.multimodal_parser.vlm_fusion import vlm_fusion_engine, CrossModalState
from ai_engine.reasoning_chain.multimodal_state import (
    multimodal_state_manager,
    MultimodalReasoningState
)
from ai_engine.reasoning_chain.engine import reasoning_engine


class UnifiedMultimodalPipeline:
    """
    ç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†ç®¡é“
    
    ç«èµ›äº®ç‚¹:
    1. âœ… VLMåŸç”Ÿèåˆ: å›¾ç‰‡åƒç´ å’Œæ–‡æœ¬åœ¨Transformerå†…éƒ¨äº¤äº’
    2. âœ… å¤šæ¨¡æ€çŠ¶æ€ç»´æŠ¤: å¯è¿½é—®æ—¶ä¿æŒè§†è§‰ä¸Šä¸‹æ–‡ä¸€è‡´æ€§
    3. âœ… æ˜¾å¼ç‰¹å¾å¯¹é½: å±•ç¤ºVision-Text Cross-Attention
    """
    
    async def process_multimodal_query(
        self,
        session_id: str,
        user_id: str,
        text: Optional[str] = None,
        image_url: Optional[str] = None,
        audio_url: Optional[str] = None,
        is_follow_up: bool = False
    ) -> Dict:
        """
        å¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢(åŒ…å«å¯è¿½é—®é€»è¾‘)
        
        ç¤ºä¾‹åœºæ™¯:
        Round 1:
        - User: [ä¸Šä¼ å›¾ç‰‡: å‡ ä½•å›¾] "è¿™ä¸ªä¸‰è§’å½¢æ€ä¹ˆæ±‚é¢ç§¯?"
        - Assistant: "æ ¹æ®å›¾ä¸­çš„åº•è¾¹5cmå’Œé«˜8cm,å¯ä»¥ç”¨å…¬å¼S=1/2Ã—bÃ—h..."
        
        Round 2 (è¿½é—®):
        - User: "é‚£å¦‚æœé«˜æœªçŸ¥å‘¢?"
        - Assistant: [è‡ªåŠ¨å…³è”Round 1çš„å›¾ç‰‡] "å¯ä»¥ç”¨å‹¾è‚¡å®šç†æ±‚é«˜..."
        """
        
        # 1. è·å–å¤šæ¨¡æ€æ¨ç†çŠ¶æ€
        state = multimodal_state_manager.get_or_create_state(session_id, user_id)
        
        # 2. å¤„ç†æŒ‡ä»£æ¶ˆè§£(å¦‚æœæ˜¯è¿½é—®)
        if is_follow_up and not image_url:
            # "é‚£ä¸ªä¸‰è§’å½¢" â†’ è§£æä¸ºä¹‹å‰ä¸Šä¼ çš„å›¾ç‰‡
            resolved_visual = state.resolve_visual_reference(text or "")
            if resolved_visual:
                image_url = resolved_visual.image_url
                print(f"ğŸ”— æŒ‡ä»£æ¶ˆè§£: å…³è”åˆ°ç¬¬{state.visual_contexts.index(resolved_visual)+1}è½®çš„å›¾ç‰‡")
        
        # 3. VLMåŸç”Ÿèåˆ(æ ¸å¿ƒæ”¹è¿›ç‚¹1)
        vlm_result = await vlm_fusion_engine.fuse_modalities(
            vision_input=image_url,
            text_input=text,
            audio_input=audio_url,
            instruction="è¯·ç†è§£è¿™ä¸ªæ•°å­¦é—®é¢˜å¹¶åˆ†æè§£é¢˜æ€è·¯"
        )
        
        # 4. æ„å»ºæ¨ç†é“¾ä¸Šä¸‹æ–‡(åŒ…å«å¤šæ¨¡æ€ä¿¡æ¯)
        context_window = state.get_context_window(
            max_turns=5,
            include_visual_context=True
        )
        
        # è½¬æ¢ä¸ºæ¨ç†å¼•æ“éœ€è¦çš„æ ¼å¼
        reasoning_context = self._build_reasoning_context(
            vlm_result,
            context_window
        )
        
        # 5. æ‰§è¡Œæ¨ç†é“¾
        reasoning_result = await reasoning_engine.reason(
            query=vlm_result["understanding"],
            context=reasoning_context,
            domain="math"
        )
        
        # 6. æ›´æ–°å¤šæ¨¡æ€çŠ¶æ€(æ ¸å¿ƒæ”¹è¿›ç‚¹2)
        turn = state.add_turn(
            user_input={
                "text": text,
                "image": image_url,
                "audio": audio_url
            },
            assistant_output={
                "content": reasoning_result["answer"],
                "visual_commands": reasoning_result.get("visual_commands", []),
                "reasoning_trace": reasoning_result["reasoning_trace"]
            },
            cross_modal_alignment=vlm_result.get("cross_modal_alignment")
        )
        
        # 7. æŒä¹…åŒ–çŠ¶æ€
        multimodal_state_manager.save_state(session_id)
        
        return {
            "session_id": session_id,
            "turn_id": turn.turn_id,
            "answer": reasoning_result["answer"],
            "reasoning_trace": reasoning_result["reasoning_trace"],
            "visual_commands": reasoning_result.get("visual_commands", []),
            
            # ç«èµ›å±•ç¤ºç‚¹: æ˜¾å¼çš„è·¨æ¨¡æ€å¯¹é½
            "cross_modal_alignment": vlm_result.get("cross_modal_alignment"),
            
            # ç«èµ›å±•ç¤ºç‚¹: å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç»´æŠ¤
            "multimodal_context_summary": {
                "total_visual_contexts": len(state.visual_contexts),
                "total_turns": len(state.turns),
                "current_active_visual": state.active_visual_index
            },
            
            "confidence": reasoning_result.get("confidence", 0.9),
            "model_used": vlm_result.get("model_used")
        }
    
    def _build_reasoning_context(
        self,
        vlm_result: Dict,
        context_window: Dict
    ) -> list:
        """
        æ„å»ºæ¨ç†é“¾ä¸Šä¸‹æ–‡
        å…³é”®: å°†å¤šæ¨¡æ€ä¿¡æ¯è½¬æ¢ä¸ºæ¨ç†å¼•æ“å¯ç”¨çš„æ ¼å¼
        """
        context = []
        
        # æ·»åŠ å†å²è½®æ¬¡
        for turn in context_window["recent_turns"]:
            context.append({
                "role": "user",
                "content": turn["user_input"].get("text", ""),
                "has_visual": bool(turn["user_input"].get("image")),
                "visual_understanding": turn.get("cross_modal_alignment", {}).get("image_understanding", "")
            })
            context.append({
                "role": "assistant",
                "content": turn["assistant_output"]["content"]
            })
        
        # æ·»åŠ å½“å‰çš„VLMç†è§£ç»“æœ
        context.append({
            "role": "system",
            "content": f"å½“å‰å¤šæ¨¡æ€ç†è§£: {vlm_result['understanding']}",
            "cross_modal_alignment": vlm_result.get("cross_modal_alignment")
        })
        
        return context


# ç¤ºä¾‹ä½¿ç”¨
async def example_multimodal_session():
    """
    æ¼”ç¤ºå®Œæ•´çš„å¤šæ¨¡æ€å¯è¿½é—®æµç¨‹
    """
    pipeline = UnifiedMultimodalPipeline()
    
    print("=" * 80)
    print("å¤šæ¨¡æ€å¯è¿½é—®æ¨ç†ç¤ºä¾‹")
    print("=" * 80)
    
    session_id = "demo_session_001"
    user_id = "student_123"
    
    # Round 1: ä¸Šä¼ å›¾ç‰‡ + æé—®
    print("\nã€Round 1ã€‘")
    print("User: [ä¸Šä¼ å‡ ä½•å›¾] è¿™ä¸ªä¸‰è§’å½¢æ€ä¹ˆæ±‚é¢ç§¯?")
    
    result1 = await pipeline.process_multimodal_query(
        session_id=session_id,
        user_id=user_id,
        text="è¿™ä¸ªä¸‰è§’å½¢æ€ä¹ˆæ±‚é¢ç§¯?",
        image_url="http://example.com/triangle.jpg",
        is_follow_up=False
    )
    
    print(f"\nAssistant: {result1['answer']}")
    print(f"\nğŸ“Š è·¨æ¨¡æ€å¯¹é½:")
    print(f"   {result1['cross_modal_alignment']}")
    print(f"\nğŸ“ˆ å¤šæ¨¡æ€ä¸Šä¸‹æ–‡: {result1['multimodal_context_summary']}")
    
    # Round 2: è¿½é—®(ä¸ä¸Šä¼ å›¾ç‰‡,ä½†è¦å¼•ç”¨Round 1çš„å›¾)
    print("\n" + "="*80)
    print("\nã€Round 2 - è¿½é—®ã€‘")
    print("User: é‚£å¦‚æœæˆ‘åªçŸ¥é“ä¸‰è¾¹é•¿åº¦å‘¢?")
    
    result2 = await pipeline.process_multimodal_query(
        session_id=session_id,
        user_id=user_id,
        text="é‚£å¦‚æœæˆ‘åªçŸ¥é“ä¸‰è¾¹é•¿åº¦å‘¢?",
        is_follow_up=True  # æ ‡è®°ä¸ºè¿½é—®
    )
    
    print(f"\nAssistant: {result2['answer']}")
    print(f"\nğŸ”— è‡ªåŠ¨å…³è”ä¸Šä¸‹æ–‡:")
    print(f"   - å¼•ç”¨äº†ç¬¬{result2['multimodal_context_summary']['current_active_visual']+1}è½®çš„å›¾ç‰‡")
    print(f"   - æ€»å…±{result2['multimodal_context_summary']['total_turns']}è½®å¯¹è¯")
    
    # Round 3: ç»§ç»­è¿½é—®
    print("\n" + "="*80)
    print("\nã€Round 3 - ç»§ç»­è¿½é—®ã€‘")
    print("User: ç”¨æµ·ä¼¦å…¬å¼æ€ä¹ˆç®—?")
    
    result3 = await pipeline.process_multimodal_query(
        session_id=session_id,
        user_id=user_id,
        text="ç”¨æµ·ä¼¦å…¬å¼æ€ä¹ˆç®—?",
        is_follow_up=True
    )
    
    print(f"\nAssistant: {result3['answer']}")
    print(f"\nâœ… å¤šè½®å¯¹è¯å®Œæˆ,è§†è§‰ä¸Šä¸‹æ–‡ä¿æŒä¸€è‡´!")


# ç”¨äºæµ‹è¯•
if __name__ == "__main__":
    import asyncio
    asyncio.run(example_multimodal_session())
