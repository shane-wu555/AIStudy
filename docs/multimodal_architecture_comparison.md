# å¤šæ¨¡æ€ç»Ÿä¸€å»ºæ¨¡æ¶æ„å¯¹æ¯”

## 1. ä¼ ç»Ÿæ¶æ„ vs æ”¹è¿›åæ¶æ„

### âŒ ä¼ ç»Ÿåˆ†ç¦»å¼æ¶æ„
```
ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ 
    â†“
OCRæå–æ–‡å­— (ç‹¬ç«‹æ¨¡å—)
    â†“
æ–‡å­— â†’ LLMæ¨ç†
    â†“
ç”Ÿæˆç­”æ¡ˆ
```

**é—®é¢˜:**
- OCRå’Œæ¨ç†åˆ†ç¦»,ä¿¡æ¯æŸå¤±
- å›¾ç‰‡çš„ç©ºé—´ç»“æ„ä¿¡æ¯ä¸¢å¤±(å¦‚å‡ ä½•å›¾å½¢çš„ä½ç½®å…³ç³»)
- å¤šè½®å¯¹è¯æ—¶,å›¾ç‰‡ä¸Šä¸‹æ–‡éš¾ä»¥ç»´æŠ¤

---

### âœ… VLMç»Ÿä¸€å»ºæ¨¡æ¶æ„ (æœ¬é¡¹ç›®å®ç°)

```
ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ + æ–‡å­—
    â†“
VLMåŸç”Ÿèåˆå¼•æ“ (vlm_fusion.py)
    â”œâ”€ å›¾ç‰‡åƒç´  â†’ Vision Encoder â†’ Image Tokens
    â”œâ”€ æ–‡å­— â†’ Text Encoder â†’ Text Tokens
    â””â”€ Cross-Modal Transformer
        â””â”€ Vision Tokens â†â†’ Text Tokens (æ³¨æ„åŠ›äº¤äº’)
    â†“
ç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨ç¤º
    â†“
æ¨ç†é“¾ç”Ÿæˆ (å«è§†è§‰ä¸Šä¸‹æ–‡)
    â†“
ç­”æ¡ˆ + è·¨æ¨¡æ€å¯¹é½ä¿¡æ¯
```

**ä¼˜åŠ¿:**
- âœ… åƒç´ çº§-æ–‡æœ¬çº§ç›´æ¥äº¤äº’
- âœ… ä¿ç•™ç©ºé—´ç»“æ„ä¿¡æ¯
- âœ… æ”¯æŒå¤æ‚çš„è§†è§‰-è¯­è¨€æ¨ç†

---

## 2. å¤šæ¨¡æ€çŠ¶æ€ç®¡ç†å¯¹æ¯”

### âŒ ä¼ ç»Ÿæ–‡æœ¬å¯¹è¯
```python
class SessionContext:
    messages: List[Dict]  # [{"role": "user", "content": "..."}]
    
    def add_message(role, content):
        messages.append({"role": role, "content": content})
```

**å±€é™:**
- åªèƒ½å­˜å‚¨æ–‡æœ¬
- è¿½é—®æ—¶æ— æ³•å¼•ç”¨ä¹‹å‰çš„å›¾ç‰‡
- æ— æ³•å¤„ç†"é‚£ä¸ªä¸‰è§’å½¢"è¿™æ ·çš„æŒ‡ä»£

---

### âœ… å¤šæ¨¡æ€æ¨ç†çŠ¶æ€ (æœ¬é¡¹ç›®)
```python
class MultimodalReasoningState:
    # å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ± 
    visual_contexts: List[VisualContext]  # æ‰€æœ‰ä¸Šä¼ çš„å›¾ç‰‡
    audio_contexts: List[AudioContext]
    
    # æ¨ç†è½®æ¬¡(åŒ…å«è·¨æ¨¡æ€å¯¹é½)
    turns: List[ReasoningTurn]
    
    def resolve_visual_reference(text):
        # "é‚£ä¸ªä¸‰è§’å½¢" â†’ æ‰¾åˆ°å¯¹åº”çš„å›¾ç‰‡
        return visual_contexts[...]
```

**ä¼˜åŠ¿:**
- âœ… ç»´æŠ¤å®Œæ•´çš„å¤šæ¨¡æ€å†å²
- âœ… æ”¯æŒæŒ‡ä»£æ¶ˆè§£
- âœ… å¯è¿½é—®æ—¶è‡ªåŠ¨å…³è”ä¹‹å‰çš„å›¾ç‰‡

---

## 3. å®é™…å¯¹è¯ç¤ºä¾‹

### åœºæ™¯: å‡ ä½•é¢˜æ±‚è§£

**Round 1:**
```
ğŸ‘¤ User: [ä¸Šä¼ å›¾ç‰‡: ä¸‰è§’å½¢ABC] "æ±‚è¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯"

ğŸ¤– Assistant (VLMèåˆ):
   - æ£€æµ‹åˆ°å›¾ç‰‡ä¸­çš„ä¸‰è§’å½¢
   - è¯†åˆ«åº•è¾¹AB=5cm, é«˜h=8cm
   - è·¨æ¨¡æ€å¯¹é½: "ä¸‰è§’å½¢" â†â†’ å›¾ç‰‡åŒºåŸŸ[100,150,300,400]
   
   ç­”æ¡ˆ: S = 1/2 Ã— 5 Ã— 8 = 20 cmÂ²
   
   [å­˜å‚¨åˆ°MultimodalReasoningState]:
   - visual_contexts[0] = {image_url, detected_objects: ["triangle"], ...}
   - turns[0] = {user_input: {image, text}, assistant_output: {...}}
```

**Round 2 (è¿½é—®,ä¸ä¸Šä¼ å›¾ç‰‡):**
```
ğŸ‘¤ User: "å¦‚æœé«˜æœªçŸ¥,åªçŸ¥é“ä¸‰è¾¹é•¿å‘¢?"

ğŸ¤– Assistant:
   1. [æŒ‡ä»£æ¶ˆè§£] "é«˜"æŒ‡çš„æ˜¯visual_contexts[0]ä¸­çš„ä¸‰è§’å½¢
   2. [è‡ªåŠ¨å…³è”å›¾ç‰‡] ä»visual_contexts[0]è·å–ä¸‰è§’å½¢ä¿¡æ¯
   3. [VLMå†æ¬¡åˆ†æ] åŸºäºå›¾ç‰‡ + æ–°é—®é¢˜æ¨ç†
   
   ç­”æ¡ˆ: å¯ä»¥ç”¨æµ·ä¼¦å…¬å¼ S = âˆš[s(s-a)(s-b)(s-c)]
   
   [æ›´æ–°çŠ¶æ€]:
   - visual_contexts[0].referenced_in_turns = [1, 2]
   - turns[1] = {user_input: {text}, assistant_output: {...}}
```

**Round 3 (ç»§ç»­è¿½é—®):**
```
ğŸ‘¤ User: "æµ·ä¼¦å…¬å¼æ€ä¹ˆæ¨å¯¼çš„?"

ğŸ¤– Assistant:
   [ä¸Šä¸‹æ–‡] å·²æœ‰2è½®å…³äºåŒä¸€ä¸‰è§’å½¢çš„å¯¹è¯
   [æ¨ç†] åŸºäºä¹‹å‰çš„è®¨è®º,ç»™å‡ºæµ·ä¼¦å…¬å¼æ¨å¯¼
```

---

## 4. ç«èµ›äº®ç‚¹æ€»ç»“

### æŠ€æœ¯åˆ›æ–°ç‚¹:

1. **VLMåŸç”Ÿèåˆ** ([vlm_fusion.py](../ai_engine/multimodal_parser/vlm_fusion.py))
   - ä½¿ç”¨Qwen-VL/GPT-4oç­‰VLM
   - å›¾ç‰‡å’Œæ–‡æœ¬åœ¨Transformerå†…éƒ¨äº¤äº’,éåå¤„ç†OCR
   - è¿”å›è·¨æ¨¡æ€å¯¹é½ä¿¡æ¯(Vision-Text Attention)

2. **å¤šæ¨¡æ€çŠ¶æ€ç»´æŠ¤** ([multimodal_state.py](../ai_engine/reasoning_chain/multimodal_state.py))
   - `MultimodalReasoningState`: ç»´æŠ¤è§†è§‰/éŸ³é¢‘ä¸Šä¸‹æ–‡
   - æŒ‡ä»£æ¶ˆè§£: "é‚£ä¸ªå›¾"è‡ªåŠ¨å…³è”åˆ°å†å²å›¾ç‰‡
   - è½®æ¬¡è¿½è¸ª: è®°å½•æ¯ä¸ªæ¨¡æ€åœ¨å“ªäº›è½®æ¬¡è¢«ä½¿ç”¨

3. **ç»Ÿä¸€æ¨ç†ç®¡é“** ([unified_multimodal_example.py](../ai_engine/examples/unified_multimodal_example.py))
   - é›†æˆVLMèåˆ + çŠ¶æ€ç®¡ç† + æ¨ç†é“¾
   - æ”¯æŒå®Œæ•´çš„å¯è¿½é—®æµç¨‹
   - è‡ªåŠ¨å¤„ç†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§

### ä¸"ä¼ ç»ŸOCR+LLM"çš„æœ¬è´¨åŒºåˆ«:

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ¡ˆ | æœ¬é¡¹ç›® |
|------|---------|--------|
| **ç‰¹å¾æå–** | OCRå…ˆè½¬æ–‡å­— | VLMåŸç”Ÿå¤„ç†åƒç´  |
| **æ¨¡æ€äº¤äº’** | åå¤„ç†æ‹¼æ¥ | Transformerå†…éƒ¨Cross-Attention |
| **ç©ºé—´ä¿¡æ¯** | ä¸¢å¤± | ä¿ç•™(bounding boxå¯¹é½) |
| **å¯è¿½é—®** | éš¾ä»¥ç»´æŠ¤å›¾ç‰‡ä¸Šä¸‹æ–‡ | MultimodalReasoningStateç®¡ç† |
| **æŒ‡ä»£æ¶ˆè§£** | ä¸æ”¯æŒ | æ”¯æŒ"é‚£ä¸ªä¸‰è§’å½¢"â†’å›¾ç‰‡æ˜ å°„ |

---

## 5. å®ç°è·¯çº¿å›¾

### Phase 1: åŸºç¡€VLMé›†æˆ âœ…
- [x] å®ç°`VLMFusionEngine`
- [x] æ”¯æŒQwen-VL/GPT-4o APIè°ƒç”¨
- [x] è¿”å›è·¨æ¨¡æ€å¯¹é½ä¿¡æ¯

### Phase 2: çŠ¶æ€ç®¡ç† âœ…
- [x] å®ç°`MultimodalReasoningState`
- [x] è§†è§‰/éŸ³é¢‘ä¸Šä¸‹æ–‡æ± 
- [x] æŒ‡ä»£æ¶ˆè§£é€»è¾‘

### Phase 3: ç«¯åˆ°ç«¯é›†æˆ âœ…
- [x] `UnifiedMultimodalPipeline`
- [x] å®Œæ•´çš„å¯è¿½é—®æµç¨‹
- [x] ç¤ºä¾‹ä»£ç 

### Phase 4: ç”Ÿäº§éƒ¨ç½² (TODO)
- [ ] RedisæŒä¹…åŒ–çŠ¶æ€
- [ ] æ€§èƒ½ä¼˜åŒ–(ç¼“å­˜VLMç»“æœ)
- [ ] ç›‘æ§å’Œæ—¥å¿—

---

## 6. å¦‚ä½•å±•ç¤ºç»™è¯„å§”

### æ¼”ç¤ºè„šæœ¬:

1. **å±•ç¤ºä¼ ç»Ÿæ–¹æ¡ˆçš„å±€é™:**
   ```
   "ä¼ ç»ŸOCRæ–¹æ¡ˆ: å›¾ç‰‡ â†’ OCRæå–'ä¸‰è§’å½¢ABC' â†’ LLMç†è§£
    é—®é¢˜: ä¸¢å¤±äº†ä¸‰è§’å½¢çš„ç©ºé—´ç»“æ„å’Œå‡ ä½•å…³ç³»"
   ```

2. **å±•ç¤ºVLMèåˆ:**
   ```
   "æˆ‘ä»¬çš„æ–¹æ¡ˆ: ä½¿ç”¨Qwen-VL,å›¾ç‰‡åƒç´ å’Œæ–‡æœ¬ç›´æ¥åœ¨Transformeräº¤äº’
    å¯ä»¥çœ‹åˆ°,ç³»ç»Ÿè¯†åˆ«å‡ºä¸‰è§’å½¢çš„ä½ç½®[100,150,300,400],
    å¹¶å°†æ–‡æœ¬ä¸­çš„'ä¸‰è§’å½¢'å’Œå›¾ç‰‡åŒºåŸŸå¯¹é½(Attention Score 0.85)"
   ```

3. **å±•ç¤ºå¯è¿½é—®:**
   ```
   Round 1: [ä¸Šä¼ å›¾ç‰‡] "æ±‚é¢ç§¯"
   Round 2: "å¦‚æœé«˜æœªçŸ¥å‘¢?" â† æ²¡æœ‰é‡æ–°ä¸Šä¼ å›¾ç‰‡
   
   ç³»ç»Ÿè‡ªåŠ¨: 
   - è§£æ"é«˜"æŒ‡çš„æ˜¯Round 1çš„ä¸‰è§’å½¢
   - ä»MultimodalReasoningStateè·å–å›¾ç‰‡ä¸Šä¸‹æ–‡
   - åŸºäºåŒä¸€å¼ å›¾ç»™å‡ºæ–°çš„æ¨ç†
   ```

4. **å±•ç¤ºçŠ¶æ€ç»´æŠ¤:**
   ```json
   {
     "visual_contexts": [
       {
         "image_url": "...",
         "referenced_in_turns": [1, 2, 3]  â† åœ¨3è½®å¯¹è¯ä¸­éƒ½è¢«å¼•ç”¨
       }
     ],
     "turns": [
       {"turn_id": 1, "user_input": {"image": "...", "text": "..."}},
       {"turn_id": 2, "user_input": {"text": "..."}},  â† æ²¡æœ‰å›¾ç‰‡,ä½†å¼•ç”¨äº†turn 1çš„
       {"turn_id": 3, "user_input": {"text": "..."}}
     ]
   }
   ```

---

## 7. å‚è€ƒæ–‡çŒ®

- **Qwen-VL**: [arxiv.org/abs/2308.12966](https://arxiv.org/abs/2308.12966)
- **GPT-4V**: [OpenAI Technical Report](https://cdn.openai.com/papers/GPTV_System_Card.pdf)
- **Cross-Modal Attention**: Attention is All You Need (Vaswani et al., 2017)
